{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d4f25a-dc3c-48f4-bcf2-f2ef01f2f71c",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4290e5-2c70-4c24-a7ae-79493493d0e7",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f9e9c-06bf-4fac-a6a3-4201348251bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010bb9c-afa6-41fa-911e-8f9cf60d8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, types\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba74240-37d1-44c2-a3b9-b57f78c19d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cleantext import clean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dacbf9-39c3-4f82-8dd2-562524dc4d97",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82d84-1a68-4cde-855d-6e956a292e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonToDF(name):\n",
    "    \"\"\"Read a list of sentences from the JSON file, store them in a dataframe\"\"\"\n",
    "    \n",
    "    with open(f\"{name}.json\") as fin:\n",
    "        textList = json.load(fin)\n",
    "\n",
    "    # create a name for each document, based on its category\n",
    "    indexNames = [f\"{name}\" for i in range(len(textList))]\n",
    "\n",
    "    # create the dataframe, it will have one column and one index\n",
    "    df = pd.DataFrame(data=textList, index=indexNames)\n",
    "    df.columns = ['document']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe9bea-8a34-4115-8c93-930884107f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = jsonToDF(\"user1\")\n",
    "user2 = jsonToDF(\"user2\")\n",
    "user3 = jsonToDF(\"user3\")\n",
    "user4 = jsonToDF(\"user4\")\n",
    "user5 = jsonToDF(\"user5\")\n",
    "user6 = jsonToDF(\"user6\")\n",
    "user7 = jsonToDF(\"user7\")\n",
    "user8 = jsonToDF(\"user8\")\n",
    "user9 = jsonToDF(\"user9\")\n",
    "user10 = jsonToDF(\"user10\")\n",
    "user3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e34969-77e7-4353-9036-2b167173cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates and missing values\n",
    "def cleanDf(df):\n",
    "    df.dropna(subset=['document'])\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983e153-7df3-4c9c-b27d-778e1f4d1adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allDocs = [user1, user2, user3, user4, user5, user6, user7, user8, user9, user10]\n",
    "for df in allDocs:\n",
    "    df = cleanDf(df)\n",
    "user3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263645f5-1746-41d8-a0cb-152396c2954a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ccedc-85ba-4ae1-b426-7cb4c9ab03ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removes emojis from text\n",
    "RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "\n",
    "def strip_emoji(text):\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    return RE_EMOJI.sub(r'', text)\n",
    "\n",
    "user3['document'] = user3['document'].apply(strip_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccc44f-41d1-4dbf-8fb9-ba95fbf624cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#splits hashtags\n",
    "def splitHashtags(sentence):\n",
    "    \"\"\"Takes a sentence and splits hashtags if present\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        hashtags = [tag.strip('#') for tag in sentence.split('#') if tag.strip('#')]\n",
    "        return ''.join(hashtags)\n",
    "    return []\n",
    "\n",
    "user3['document'] = user3['document'].apply(splitHashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93dfabb-a485-4d42-908c-e1b29773df9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toRemove = ['fyp', 'trending', 'foryou', 'viral', 'foryoupage', 'fy', 'fypage', 'blowthisup', 'tiktok', 'video', 'videos', 'forypu', 'fup', 'everyonefyp', 'reaction', 'fypviral', 'relatable']\n",
    "\n",
    "def remove_words(sentence):\n",
    "    \"\"\"Takes a sentence and removes popular phrases\"\"\"\n",
    "    if not isinstance(sentence, str):\n",
    "        return str(sentence)\n",
    "    words = sentence.split()\n",
    "    return ' '.join(word for word in words if word.lower() not in toRemove)\n",
    "\n",
    "user3['document'] = user3['document'].apply(remove_words)\n",
    "pd.set_option(\"display.max_colwidth\",1000)\n",
    "user3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca96ef-0792-4e69-965d-eae6e3332ea9",
   "metadata": {},
   "source": [
    "### Convert to document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11deb981-8c74-47c2-855f-4ab27b7948c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86434a36-e195-442c-81a1-40da2a903844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    token_pattern=r'\\b[a-zA-Z]{3,}\\b', # we want only words that contain letters and are 3 or more characters long\n",
    ")\n",
    "\n",
    "# Transform our data into the document-term matrix\n",
    "dtm = vectorizer.fit_transform(user7['document'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f9f1b-3507-47e8-978b-a2035a306134",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc25ef0-2211-4477-a4bf-d6007c7cd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix2Doc(dtMatrix, features, index):\n",
    "    \"\"\"Turns each row of the document-term matrix into a list of terms\"\"\"\n",
    "    row = dtMatrix.getrow(index).toarray()\n",
    "    non_zero_indices = row.nonzero()[1]\n",
    "    words = [features[idx] for idx in non_zero_indices]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f7633-97f7-4fe3-9f8b-5060af6bbc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "user7AsTerms = [matrix2Doc(dtm, feature_names, i) for i in range(dtm.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8455a30-6070-49be-bc71-3c20f4960a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user7['terms'] = user7AsTerms\n",
    "pd.set_option(\"display.max_colwidth\",1000)\n",
    "user7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce92542-061a-4259-8aea-2f22db03fc07",
   "metadata": {},
   "source": [
    "### Fitting the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220c919-1ac2-4b1f-8c4b-24d1186219db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6449bd7-dc86-4704-9262-78bdfc1b1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the model\n",
    "lda = LatentDirichletAllocation(n_components=5, # we are picking the number of topics arbitrarily at the moment\n",
    "                                random_state=0)\n",
    "# Step 2: Fit the model\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e9a68-87cd-4bfa-b92d-9c43c5e50794",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b886d-ca5f-4419-accd-b01b68b0eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_dist = lda.transform(dtm)\n",
    "doc_topic_dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570f070-9935-4764-b31d-37a25dc82068",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ca1cf-4f57-43ad-bf26-eb4faf140be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, features, no_top_words):\n",
    "    \"\"\"Helper function to show the top words of a model\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):                                                                                                             \n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([features[i]\n",
    "                        for i in topic.argsort()[:-no_top_words-1:-1]]))\n",
    "\n",
    "display_topics(lda, feature_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831802c6-e4a4-4389-a08d-78d6d4c63267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayHeader(model, features, no_top_words):\n",
    "    \"\"\"Helper function to show the top words of a model\"\"\"\n",
    "    topicNames = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topicNames.append(f\"Topic {topic_idx}: \" + (\", \".join([features[i]\n",
    "                             for i in topic.argsort()[:-no_top_words-1:-1]])))\n",
    "    return topicNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1ce06-37ce-4d6f-802c-f9bbea664650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names\n",
    "topicnames = displayHeader(lda, feature_names, 0)\n",
    "\n",
    "# index names\n",
    "docnames = user8.index.tolist() # We will use the original names of the documents\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(doc_topic_dist, 3), \n",
    "                                 columns=topicnames, \n",
    "                                 index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1) # finds the maximum argument\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "df_document_topic.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74b003-77e5-4831-8212-6b1fe806c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Sentences']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae20d8-48b6-49fe-98e2-7d463ee2c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from DataFrame columns\n",
    "x_values = df_topic_distribution['Topic Num']\n",
    "y_values = df_topic_distribution['Num Sentences']\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.style.use('classic')\n",
    "plt.bar(x_values, y_values, color='darkred')\n",
    "plt.grid(axis='y', linestyle='solid', linewidth=0.5, alpha=0.5)\n",
    "plt.xlim(-0.5, 4.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Sent ence Counts')\n",
    "plt.title('user8 Bar Chart of Topics vs. Sentences')\n",
    "\n",
    "# Rotate x-axis labels for better readability (if needed)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
